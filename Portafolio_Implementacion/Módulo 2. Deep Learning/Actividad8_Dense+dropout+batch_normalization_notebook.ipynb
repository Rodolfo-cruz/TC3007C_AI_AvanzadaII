{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realizado por:** Rodolfo Jesús Cruz Rebollar\n",
    "\n",
    "**Matrícula:** A01368326\n",
    "\n",
    "**Grupo:** 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "You are a data scientist working for a school\n",
    "\n",
    "You are asked to predict the GPA of the current students based on the following provided data: \n",
    "\n",
    " 0   StudentID  int64  \n",
    " 1   Age    int64  \n",
    " 2   Gender int64  \n",
    " 3   Ethnicity  int64  \n",
    " 4   ParentalEducation  int64  \n",
    " 5   StudyTimeWeekly    float64\n",
    " 6   Absences   int64  \n",
    " 7   Tutoring   int64  \n",
    " 8   ParentalSupport    int64  \n",
    " 9   Extracurricular    int64  \n",
    " 10  Sports int64  \n",
    " 11  Music  int64  \n",
    " 12  Volunteering   int64  \n",
    " 13  GPA    float64\n",
    " 14  GradeClass float64\n",
    "\n",
    "The GPA is the Grade Point Average, typically ranges from 0.0 to 4.0 in most educational systems, with 4.0 representing an 'A' or excellent performance.\n",
    "\n",
    "The minimum passing GPA can vary by institution, but it's often around 2.0. This usually corresponds to a 'C' grade, which is considered satisfactory.\n",
    "\n",
    "You need to create a Deep Learning model capable to predict the GPA of a Student based on a set of provided features.\n",
    "The data provided represents 2,392 students.\n",
    "\n",
    "In this excersice you will be requested to create a total of three models and select the most performant one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import Libraries\n",
    "\n",
    "First let's import the following libraries, if there is any library that you need and is not in the list bellow feel free to include it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rodolfo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudentID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>ParentalEducation</th>\n",
       "      <th>StudyTimeWeekly</th>\n",
       "      <th>Absences</th>\n",
       "      <th>Tutoring</th>\n",
       "      <th>ParentalSupport</th>\n",
       "      <th>Extracurricular</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Music</th>\n",
       "      <th>Volunteering</th>\n",
       "      <th>GPA</th>\n",
       "      <th>GradeClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19.833723</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.929196</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.408756</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.042915</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.210570</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112602</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.028829</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.054218</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.672495</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.288061</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>3388</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.680555</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.455509</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>3389</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.583217</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.279150</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>3390</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.805500</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.142333</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>3391</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.416653</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.803297</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>3392</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.819907</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.140014</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2392 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      StudentID  Age  Gender  Ethnicity  ParentalEducation  StudyTimeWeekly  \\\n",
       "0          1001   17       1          0                  2        19.833723   \n",
       "1          1002   18       0          0                  1        15.408756   \n",
       "2          1003   15       0          2                  3         4.210570   \n",
       "3          1004   17       1          0                  3        10.028829   \n",
       "4          1005   17       1          0                  2         4.672495   \n",
       "...         ...  ...     ...        ...                ...              ...   \n",
       "2387       3388   18       1          0                  3        10.680555   \n",
       "2388       3389   17       0          0                  1         7.583217   \n",
       "2389       3390   16       1          0                  2         6.805500   \n",
       "2390       3391   16       1          1                  0        12.416653   \n",
       "2391       3392   16       1          0                  2        17.819907   \n",
       "\n",
       "      Absences  Tutoring  ParentalSupport  Extracurricular  Sports  Music  \\\n",
       "0            7         1                2                0       0      1   \n",
       "1            0         0                1                0       0      0   \n",
       "2           26         0                2                0       0      0   \n",
       "3           14         0                3                1       0      0   \n",
       "4           17         1                3                0       0      0   \n",
       "...        ...       ...              ...              ...     ...    ...   \n",
       "2387         2         0                4                1       0      0   \n",
       "2388         4         1                4                0       1      0   \n",
       "2389        20         0                2                0       0      0   \n",
       "2390        17         0                2                0       1      1   \n",
       "2391        13         0                2                0       0      0   \n",
       "\n",
       "      Volunteering       GPA  GradeClass  \n",
       "0                0  2.929196         2.0  \n",
       "1                0  3.042915         1.0  \n",
       "2                0  0.112602         4.0  \n",
       "3                0  2.054218         3.0  \n",
       "4                0  1.288061         4.0  \n",
       "...            ...       ...         ...  \n",
       "2387             0  3.455509         0.0  \n",
       "2388             0  3.279150         4.0  \n",
       "2389             1  1.142333         2.0  \n",
       "2390             0  1.803297         1.0  \n",
       "2391             1  2.140014         1.0  \n",
       "\n",
       "[2392 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Student_performance_data _.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Review you data:\n",
    "\n",
    "Make sure you review your data.\n",
    "Place special attention of null or empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2392 entries, 0 to 2391\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   StudentID          2392 non-null   int64  \n",
      " 1   Age                2392 non-null   int64  \n",
      " 2   Gender             2392 non-null   int64  \n",
      " 3   Ethnicity          2392 non-null   int64  \n",
      " 4   ParentalEducation  2392 non-null   int64  \n",
      " 5   StudyTimeWeekly    2392 non-null   float64\n",
      " 6   Absences           2392 non-null   int64  \n",
      " 7   Tutoring           2392 non-null   int64  \n",
      " 8   ParentalSupport    2392 non-null   int64  \n",
      " 9   Extracurricular    2392 non-null   int64  \n",
      " 10  Sports             2392 non-null   int64  \n",
      " 11  Music              2392 non-null   int64  \n",
      " 12  Volunteering       2392 non-null   int64  \n",
      " 13  GPA                2392 non-null   float64\n",
      " 14  GradeClass         2392 non-null   float64\n",
      "dtypes: float64(3), int64(12)\n",
      "memory usage: 280.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove the columns not needed for Student performance prediction\n",
    "\n",
    "- Choose only the columns you consider to be valuable for your model training.\n",
    "- For example, StudentID might not be a good feature for your model, and thus should be removed from your main dataset, which other columns should also be removed?\n",
    "- You can name that final dataset as 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>ParentalEducation</th>\n",
       "      <th>StudyTimeWeekly</th>\n",
       "      <th>Absences</th>\n",
       "      <th>ParentalSupport</th>\n",
       "      <th>Sports</th>\n",
       "      <th>GradeClass</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>19.833723</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.929196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>15.408756</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.042915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4.210570</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.112602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>10.028829</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.054218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>4.672495</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.288061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  ParentalEducation  StudyTimeWeekly  Absences  ParentalSupport  Sports  \\\n",
       "0   17                  2        19.833723         7                2       0   \n",
       "1   18                  1        15.408756         0                1       0   \n",
       "2   15                  3         4.210570        26                2       0   \n",
       "3   17                  3        10.028829        14                3       0   \n",
       "4   17                  2         4.672495        17                3       0   \n",
       "\n",
       "   GradeClass       GPA  \n",
       "0         2.0  2.929196  \n",
       "1         1.0  3.042915  \n",
       "2         4.0  0.112602  \n",
       "3         3.0  2.054218  \n",
       "4         4.0  1.288061  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Columnas relevantes a seleccionar: Age, ParentalEducation, StudyTimeWeekly, Absenses, \n",
    "   ParentalSupport, Sports, GradeClass, GPA\"\"\"\n",
    "\n",
    "dataset = data.iloc[:, [1, 4, 5, 6, 8, 10, 14, 13]]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Check if the columns has any null values:\n",
    "- Here you now have your final dataset to use in your model training.\n",
    "- Before moving foward review your data check for any null or empty value that might be needed to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                  False\n",
       "ParentalEducation    False\n",
       "StudyTimeWeekly      False\n",
       "Absences             False\n",
       "ParentalSupport      False\n",
       "Sports               False\n",
       "GradeClass           False\n",
       "GPA                  False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisar si existen valores nulos o faltantes en cada columna del dataset\n",
    "\n",
    "dataset.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare your data for training and for testing set:\n",
    " - First create a dataset named X, with all columns but GPA. These are the features\n",
    " - Next create another dataset named y, with only GPA column. This is the label\n",
    " - If you go to your Imports, you will see the following import: **'from sklearn.model_selection import train_test_split'**\n",
    " - Use that *train_test_split* function to create: X_train, X_test, y_train and y_test respectively. Use X and y datasets as parameters. Other parameters to use are: Test Size = 0.2, Random State = 42.\n",
    " \n",
    " - Standarize your features (X_train and X_test) by using the StandardScaler (investigate how to use fit_transform and transform functions). This will help the training process by dealing with normilized data.\n",
    "\n",
    " Note: Your X_train shape should be around (1913, 10). This means the dataset has 10 columns which should be the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.37285117,  2.26211643,  1.46815853, ..., -1.9146563 ,\n",
       "         1.51039849,  0.82018081],\n",
       "       [-0.40585814,  0.23853507, -1.27677348, ..., -1.02021491,\n",
       "        -0.66207693,  0.82018081],\n",
       "       [ 0.48349652,  1.25032575, -1.10363153, ..., -1.9146563 ,\n",
       "        -0.66207693,  0.82018081],\n",
       "       ...,\n",
       "       [-0.40585814,  0.23853507, -1.08325424, ..., -0.12577352,\n",
       "         1.51039849,  0.01141489],\n",
       "       [ 1.37285117,  0.23853507, -0.93767144, ...,  0.76866788,\n",
       "        -0.66207693,  0.82018081],\n",
       "       [ 1.37285117,  0.23853507, -0.7578795 , ..., -0.12577352,\n",
       "        -0.66207693,  0.82018081]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset solo con variables predictoras sin la variable respuesta GPA\n",
    "\n",
    "X = dataset.iloc[:, :7]\n",
    "\n",
    "# Dataset solo con la variable respuesta GPA\n",
    "\n",
    "y = dataset[\"GPA\"]\n",
    "\n",
    "# Crear conjuntos de entrenamiento y prueba para X, y\n",
    "\n",
    "# Destinar 20% de los datos para probar el modelo y el 80% sobrante para entrenarlo\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Estandarizar/normalizar los valores de las variables sin considerar a la variable respuesta (GPA)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "std_features = scaler.fit_transform(X_train, X_test)\n",
    "\n",
    "std_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Different Neural Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** se utilizará un valor de dropout igual a 0.5 para los modelos 3 y 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: A single Dense Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rodolfo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: a single dense hidden layer\n",
    "\n",
    "E1 = Sequential()\n",
    "\n",
    "# Agregar capa de entrada con 64 unidades y activación Relu y dimensión de entrada = 7\n",
    "\n",
    "E1.add(Dense(64, activation=\"relu\", input_dim = 7))\n",
    "\n",
    "# Agregar capa densa oculta simple on 32 unidades y función de activación Relu\n",
    "\n",
    "E1.add(Dense(32, activation=\"relu\")) # single dense hidden layer\n",
    "\n",
    "# Añadir capa de salida con 1 unidad \n",
    "\n",
    "E1.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo 1\n",
    "\n",
    "E1.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.4415 - mae: 0.7374 - val_loss: 0.1245 - val_mae: 0.2772\n",
      "Epoch 2/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1208 - mae: 0.2763 - val_loss: 0.1206 - val_mae: 0.2810\n",
      "Epoch 3/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1037 - mae: 0.2628 - val_loss: 0.1306 - val_mae: 0.2953\n",
      "Epoch 4/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1067 - mae: 0.2661 - val_loss: 0.1083 - val_mae: 0.2636\n",
      "Epoch 5/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1097 - mae: 0.2660 - val_loss: 0.1131 - val_mae: 0.2734\n",
      "Epoch 6/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0986 - mae: 0.2502 - val_loss: 0.0924 - val_mae: 0.2438\n",
      "Epoch 7/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0929 - mae: 0.2446 - val_loss: 0.1059 - val_mae: 0.2624\n",
      "Epoch 8/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0942 - mae: 0.2486 - val_loss: 0.0959 - val_mae: 0.2505\n",
      "Epoch 9/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0883 - mae: 0.2388 - val_loss: 0.0981 - val_mae: 0.2533\n",
      "Epoch 10/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1028 - mae: 0.2536 - val_loss: 0.0879 - val_mae: 0.2400\n",
      "Epoch 11/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0915 - mae: 0.2464 - val_loss: 0.1296 - val_mae: 0.2895\n",
      "Epoch 12/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0880 - mae: 0.2341 - val_loss: 0.1150 - val_mae: 0.2711\n",
      "Epoch 13/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0885 - mae: 0.2409 - val_loss: 0.0868 - val_mae: 0.2371\n",
      "Epoch 14/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0808 - mae: 0.2270 - val_loss: 0.1074 - val_mae: 0.2658\n",
      "Epoch 15/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0899 - mae: 0.2424 - val_loss: 0.0863 - val_mae: 0.2338\n",
      "Epoch 16/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0868 - mae: 0.2321 - val_loss: 0.0821 - val_mae: 0.2269\n",
      "Epoch 17/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0777 - mae: 0.2245 - val_loss: 0.1180 - val_mae: 0.2836\n",
      "Epoch 18/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0829 - mae: 0.2306 - val_loss: 0.0790 - val_mae: 0.2254\n",
      "Epoch 19/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0709 - mae: 0.2144 - val_loss: 0.1200 - val_mae: 0.2776\n",
      "Epoch 20/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0842 - mae: 0.2337 - val_loss: 0.0727 - val_mae: 0.2138\n",
      "Epoch 21/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0757 - mae: 0.2221 - val_loss: 0.0860 - val_mae: 0.2356\n",
      "Epoch 22/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0745 - mae: 0.2174 - val_loss: 0.0715 - val_mae: 0.2119\n",
      "Epoch 23/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0783 - mae: 0.2234 - val_loss: 0.0965 - val_mae: 0.2525\n",
      "Epoch 24/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0858 - mae: 0.2312 - val_loss: 0.0748 - val_mae: 0.2166\n",
      "Epoch 25/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0800 - mae: 0.2242 - val_loss: 0.0737 - val_mae: 0.2173\n",
      "Epoch 26/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0686 - mae: 0.2090 - val_loss: 0.0688 - val_mae: 0.2076\n",
      "Epoch 27/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0693 - mae: 0.2141 - val_loss: 0.0770 - val_mae: 0.2203\n",
      "Epoch 28/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0686 - mae: 0.2092 - val_loss: 0.0854 - val_mae: 0.2326\n",
      "Epoch 29/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0738 - mae: 0.2151 - val_loss: 0.0688 - val_mae: 0.2087\n",
      "Epoch 30/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0692 - mae: 0.2081 - val_loss: 0.0669 - val_mae: 0.2064\n",
      "Epoch 31/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0634 - mae: 0.2010 - val_loss: 0.0814 - val_mae: 0.2259\n",
      "Epoch 32/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0830 - mae: 0.2311 - val_loss: 0.0790 - val_mae: 0.2304\n",
      "Epoch 33/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0663 - mae: 0.2071 - val_loss: 0.1089 - val_mae: 0.2715\n",
      "Epoch 34/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0749 - mae: 0.2193 - val_loss: 0.0755 - val_mae: 0.2222\n",
      "Epoch 35/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0702 - mae: 0.2115 - val_loss: 0.0687 - val_mae: 0.2105\n",
      "Epoch 36/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0628 - mae: 0.1975 - val_loss: 0.0786 - val_mae: 0.2221\n",
      "Epoch 37/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0692 - mae: 0.2114 - val_loss: 0.0881 - val_mae: 0.2386\n",
      "Epoch 38/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0696 - mae: 0.2116 - val_loss: 0.0766 - val_mae: 0.2252\n",
      "Epoch 39/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0669 - mae: 0.2045 - val_loss: 0.0882 - val_mae: 0.2389\n",
      "Epoch 40/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0634 - mae: 0.2011 - val_loss: 0.0855 - val_mae: 0.2356\n",
      "Epoch 41/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0657 - mae: 0.2023 - val_loss: 0.0667 - val_mae: 0.2052\n",
      "Epoch 42/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0629 - mae: 0.1993 - val_loss: 0.0636 - val_mae: 0.2030\n",
      "Epoch 43/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0622 - mae: 0.1979 - val_loss: 0.0627 - val_mae: 0.2003\n",
      "Epoch 44/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0564 - mae: 0.1892 - val_loss: 0.0918 - val_mae: 0.2491\n",
      "Epoch 45/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0661 - mae: 0.2054 - val_loss: 0.0609 - val_mae: 0.1985\n",
      "Epoch 46/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0596 - mae: 0.1935 - val_loss: 0.0621 - val_mae: 0.1995\n",
      "Epoch 47/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0628 - mae: 0.1996 - val_loss: 0.0847 - val_mae: 0.2402\n",
      "Epoch 48/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0626 - mae: 0.1970 - val_loss: 0.0689 - val_mae: 0.2119\n",
      "Epoch 49/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0586 - mae: 0.1917 - val_loss: 0.0659 - val_mae: 0.2045\n",
      "Epoch 50/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0606 - mae: 0.1965 - val_loss: 0.0579 - val_mae: 0.1919\n",
      "Epoch 51/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0562 - mae: 0.1912 - val_loss: 0.0692 - val_mae: 0.2136\n",
      "Epoch 52/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0519 - mae: 0.1810 - val_loss: 0.0684 - val_mae: 0.2135\n",
      "Epoch 53/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0580 - mae: 0.1922 - val_loss: 0.0594 - val_mae: 0.1931\n",
      "Epoch 54/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0619 - mae: 0.1979 - val_loss: 0.0662 - val_mae: 0.2035\n",
      "Epoch 55/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0609 - mae: 0.1943 - val_loss: 0.0558 - val_mae: 0.1884\n",
      "Epoch 56/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0558 - mae: 0.1892 - val_loss: 0.0672 - val_mae: 0.2046\n",
      "Epoch 57/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0610 - mae: 0.1965 - val_loss: 0.0565 - val_mae: 0.1886\n",
      "Epoch 58/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0540 - mae: 0.1843 - val_loss: 0.0614 - val_mae: 0.1975\n",
      "Epoch 59/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0514 - mae: 0.1792 - val_loss: 0.0652 - val_mae: 0.2077\n",
      "Epoch 60/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0584 - mae: 0.1908 - val_loss: 0.0817 - val_mae: 0.2341\n",
      "Epoch 61/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0567 - mae: 0.1898 - val_loss: 0.0652 - val_mae: 0.2084\n",
      "Epoch 62/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0542 - mae: 0.1862 - val_loss: 0.0701 - val_mae: 0.2076\n",
      "Epoch 63/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0540 - mae: 0.1834 - val_loss: 0.0667 - val_mae: 0.2132\n",
      "Epoch 64/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0518 - mae: 0.1792 - val_loss: 0.0666 - val_mae: 0.2018\n",
      "Epoch 65/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0630 - mae: 0.2008 - val_loss: 0.0713 - val_mae: 0.2120\n",
      "Epoch 66/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0542 - mae: 0.1864 - val_loss: 0.0577 - val_mae: 0.1909\n",
      "Epoch 67/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0546 - mae: 0.1856 - val_loss: 0.0584 - val_mae: 0.1963\n",
      "Epoch 68/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496 - mae: 0.1768 - val_loss: 0.0805 - val_mae: 0.2240\n",
      "Epoch 69/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0542 - mae: 0.1877 - val_loss: 0.0579 - val_mae: 0.1878\n",
      "Epoch 70/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0511 - mae: 0.1794 - val_loss: 0.0601 - val_mae: 0.1982\n",
      "Epoch 71/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0557 - mae: 0.1866 - val_loss: 0.0584 - val_mae: 0.1965\n",
      "Epoch 72/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0496 - mae: 0.1721 - val_loss: 0.0622 - val_mae: 0.2001\n",
      "Epoch 73/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0481 - mae: 0.1739 - val_loss: 0.0697 - val_mae: 0.2077\n",
      "Epoch 74/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0552 - mae: 0.1837 - val_loss: 0.0601 - val_mae: 0.1947\n",
      "Epoch 75/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501 - mae: 0.1774 - val_loss: 0.0574 - val_mae: 0.1908\n",
      "Epoch 76/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0503 - mae: 0.1769 - val_loss: 0.0591 - val_mae: 0.1909\n",
      "Epoch 77/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0460 - mae: 0.1684 - val_loss: 0.0627 - val_mae: 0.1965\n",
      "Epoch 78/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0479 - mae: 0.1722 - val_loss: 0.0590 - val_mae: 0.1934\n",
      "Epoch 79/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506 - mae: 0.1767 - val_loss: 0.0563 - val_mae: 0.1872\n",
      "Epoch 80/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0517 - mae: 0.1799 - val_loss: 0.0655 - val_mae: 0.2124\n",
      "Epoch 81/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0488 - mae: 0.1736 - val_loss: 0.0531 - val_mae: 0.1816\n",
      "Epoch 82/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0451 - mae: 0.1665 - val_loss: 0.0678 - val_mae: 0.2169\n",
      "Epoch 83/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0561 - mae: 0.1870 - val_loss: 0.0701 - val_mae: 0.2058\n",
      "Epoch 84/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0507 - mae: 0.1787 - val_loss: 0.0696 - val_mae: 0.2127\n",
      "Epoch 85/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0510 - mae: 0.1795 - val_loss: 0.0559 - val_mae: 0.1903\n",
      "Epoch 86/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0477 - mae: 0.1717 - val_loss: 0.0529 - val_mae: 0.1812\n",
      "Epoch 87/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0481 - mae: 0.1721 - val_loss: 0.0559 - val_mae: 0.1893\n",
      "Epoch 88/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0474 - mae: 0.1718 - val_loss: 0.0588 - val_mae: 0.1912\n",
      "Epoch 89/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0493 - mae: 0.1764 - val_loss: 0.0529 - val_mae: 0.1836\n",
      "Epoch 90/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0446 - mae: 0.1695 - val_loss: 0.0853 - val_mae: 0.2254\n",
      "Epoch 91/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0464 - mae: 0.1667 - val_loss: 0.0671 - val_mae: 0.2124\n",
      "Epoch 92/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0511 - mae: 0.1784 - val_loss: 0.0545 - val_mae: 0.1864\n",
      "Epoch 93/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0524 - mae: 0.1785 - val_loss: 0.0537 - val_mae: 0.1853\n",
      "Epoch 94/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1632 - val_loss: 0.0552 - val_mae: 0.1893\n",
      "Epoch 95/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0484 - mae: 0.1732 - val_loss: 0.0578 - val_mae: 0.1890\n",
      "Epoch 96/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0492 - mae: 0.1724 - val_loss: 0.0598 - val_mae: 0.1996\n",
      "Epoch 97/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0458 - mae: 0.1692 - val_loss: 0.0536 - val_mae: 0.1844\n",
      "Epoch 98/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0440 - mae: 0.1633 - val_loss: 0.0522 - val_mae: 0.1786\n",
      "Epoch 99/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1664 - val_loss: 0.0499 - val_mae: 0.1759\n",
      "Epoch 100/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1646 - val_loss: 0.0582 - val_mae: 0.1939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e0a281cac0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del modelo 1\n",
    "\n",
    "E1.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0587 - mae: 0.1782 \n",
      "Valores de pérdida modelo 1: [0.05687776580452919, 0.1798904687166214]\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo 1\n",
    "\n",
    "eval1 = E1.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Valores de pérdida modelo 1: {eval1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2: A set of three Dense Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: a single dense hidden layer\n",
    "\n",
    "E2 = Sequential()\n",
    "\n",
    "# Agregar capa de entrada con 64 unidades y activación Relu y dimensión de entrada = 7\n",
    "\n",
    "E2.add(Dense(64, activation=\"relu\", input_dim = 7))\n",
    "\n",
    "# Agregar capa densa oculta 1 con 32 unidades y función de activación Relu\n",
    "\n",
    "E2.add(Dense(32, activation=\"relu\")) # dense hidden layer 1\n",
    "\n",
    "# Agregar capa densa oculta 2 con 16 unidades y función de activación Relu\n",
    "\n",
    "E2.add(Dense(16, activation=\"relu\")) # dense hidden layer 2\n",
    "\n",
    "# Agregar capa densa oculta 3 con 8 unidades y función de activación Relu\n",
    "\n",
    "E2.add(Dense(8, activation=\"relu\")) # dense hidden layer 3\n",
    "\n",
    "# Añadir capa de salida con 1 unidad \n",
    "\n",
    "E2.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo 2 (con 3 capas densas ocultas)\n",
    "\n",
    "E2.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.0956 - mae: 0.6838 - val_loss: 0.1627 - val_mae: 0.3234\n",
      "Epoch 2/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1252 - mae: 0.2833 - val_loss: 0.1401 - val_mae: 0.3048\n",
      "Epoch 3/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0974 - mae: 0.2575 - val_loss: 0.1196 - val_mae: 0.2815\n",
      "Epoch 4/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0999 - mae: 0.2539 - val_loss: 0.1139 - val_mae: 0.2712\n",
      "Epoch 5/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0912 - mae: 0.2432 - val_loss: 0.1526 - val_mae: 0.3190\n",
      "Epoch 6/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1055 - mae: 0.2603 - val_loss: 0.1051 - val_mae: 0.2596\n",
      "Epoch 7/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0904 - mae: 0.2425 - val_loss: 0.0917 - val_mae: 0.2423\n",
      "Epoch 8/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0874 - mae: 0.2375 - val_loss: 0.1007 - val_mae: 0.2557\n",
      "Epoch 9/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0876 - mae: 0.2348 - val_loss: 0.0982 - val_mae: 0.2522\n",
      "Epoch 10/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0847 - mae: 0.2305 - val_loss: 0.0983 - val_mae: 0.2533\n",
      "Epoch 11/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0789 - mae: 0.2264 - val_loss: 0.0988 - val_mae: 0.2550\n",
      "Epoch 12/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0806 - mae: 0.2306 - val_loss: 0.0920 - val_mae: 0.2435\n",
      "Epoch 13/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0786 - mae: 0.2248 - val_loss: 0.0844 - val_mae: 0.2324\n",
      "Epoch 14/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0724 - mae: 0.2159 - val_loss: 0.0982 - val_mae: 0.2544\n",
      "Epoch 15/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0781 - mae: 0.2255 - val_loss: 0.0840 - val_mae: 0.2320\n",
      "Epoch 16/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0724 - mae: 0.2156 - val_loss: 0.0801 - val_mae: 0.2266\n",
      "Epoch 17/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0736 - mae: 0.2156 - val_loss: 0.0808 - val_mae: 0.2277\n",
      "Epoch 18/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0766 - mae: 0.2210 - val_loss: 0.0803 - val_mae: 0.2276\n",
      "Epoch 19/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0681 - mae: 0.2093 - val_loss: 0.0938 - val_mae: 0.2463\n",
      "Epoch 20/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0745 - mae: 0.2201 - val_loss: 0.0862 - val_mae: 0.2354\n",
      "Epoch 21/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0744 - mae: 0.2156 - val_loss: 0.0827 - val_mae: 0.2322\n",
      "Epoch 22/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0700 - mae: 0.2097 - val_loss: 0.0839 - val_mae: 0.2333\n",
      "Epoch 23/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0691 - mae: 0.2116 - val_loss: 0.0825 - val_mae: 0.2308\n",
      "Epoch 24/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0643 - mae: 0.2036 - val_loss: 0.0959 - val_mae: 0.2467\n",
      "Epoch 25/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0684 - mae: 0.2091 - val_loss: 0.0831 - val_mae: 0.2299\n",
      "Epoch 26/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0684 - mae: 0.2062 - val_loss: 0.0698 - val_mae: 0.2107\n",
      "Epoch 27/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0690 - mae: 0.2061 - val_loss: 0.0720 - val_mae: 0.2146\n",
      "Epoch 28/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0605 - mae: 0.1952 - val_loss: 0.0749 - val_mae: 0.2190\n",
      "Epoch 29/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0578 - mae: 0.1921 - val_loss: 0.0704 - val_mae: 0.2123\n",
      "Epoch 30/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0646 - mae: 0.2018 - val_loss: 0.0766 - val_mae: 0.2221\n",
      "Epoch 31/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0628 - mae: 0.1992 - val_loss: 0.0728 - val_mae: 0.2136\n",
      "Epoch 32/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0620 - mae: 0.1992 - val_loss: 0.0687 - val_mae: 0.2089\n",
      "Epoch 33/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0602 - mae: 0.1946 - val_loss: 0.0669 - val_mae: 0.2054\n",
      "Epoch 34/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0581 - mae: 0.1926 - val_loss: 0.0676 - val_mae: 0.2091\n",
      "Epoch 35/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0585 - mae: 0.1933 - val_loss: 0.0671 - val_mae: 0.2049\n",
      "Epoch 36/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0594 - mae: 0.1953 - val_loss: 0.0626 - val_mae: 0.1987\n",
      "Epoch 37/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0580 - mae: 0.1935 - val_loss: 0.0680 - val_mae: 0.2070\n",
      "Epoch 38/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0625 - mae: 0.1977 - val_loss: 0.0689 - val_mae: 0.2096\n",
      "Epoch 39/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0573 - mae: 0.1893 - val_loss: 0.0671 - val_mae: 0.2079\n",
      "Epoch 40/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0506 - mae: 0.1747 - val_loss: 0.0609 - val_mae: 0.1980\n",
      "Epoch 41/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0585 - mae: 0.1919 - val_loss: 0.0661 - val_mae: 0.2060\n",
      "Epoch 42/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0584 - mae: 0.1906 - val_loss: 0.0623 - val_mae: 0.1975\n",
      "Epoch 43/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0568 - mae: 0.1903 - val_loss: 0.0616 - val_mae: 0.1985\n",
      "Epoch 44/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0618 - mae: 0.1949 - val_loss: 0.0718 - val_mae: 0.2196\n",
      "Epoch 45/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0587 - mae: 0.1934 - val_loss: 0.0762 - val_mae: 0.2189\n",
      "Epoch 46/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0521 - mae: 0.1760 - val_loss: 0.0644 - val_mae: 0.2041\n",
      "Epoch 47/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0524 - mae: 0.1809 - val_loss: 0.0620 - val_mae: 0.1980\n",
      "Epoch 48/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0565 - mae: 0.1884 - val_loss: 0.0556 - val_mae: 0.1883\n",
      "Epoch 49/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0530 - mae: 0.1818 - val_loss: 0.0568 - val_mae: 0.1895\n",
      "Epoch 50/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0481 - mae: 0.1728 - val_loss: 0.0681 - val_mae: 0.2132\n",
      "Epoch 51/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0569 - mae: 0.1884 - val_loss: 0.0581 - val_mae: 0.1926\n",
      "Epoch 52/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499 - mae: 0.1748 - val_loss: 0.0576 - val_mae: 0.1903\n",
      "Epoch 53/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0511 - mae: 0.1752 - val_loss: 0.0629 - val_mae: 0.1963\n",
      "Epoch 54/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0500 - mae: 0.1742 - val_loss: 0.0550 - val_mae: 0.1839\n",
      "Epoch 55/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0480 - mae: 0.1719 - val_loss: 0.0647 - val_mae: 0.1984\n",
      "Epoch 56/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0465 - mae: 0.1689 - val_loss: 0.0552 - val_mae: 0.1852\n",
      "Epoch 57/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0519 - mae: 0.1778 - val_loss: 0.0582 - val_mae: 0.1887\n",
      "Epoch 58/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0479 - mae: 0.1727 - val_loss: 0.0548 - val_mae: 0.1894\n",
      "Epoch 59/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499 - mae: 0.1762 - val_loss: 0.0591 - val_mae: 0.1951\n",
      "Epoch 60/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0490 - mae: 0.1735 - val_loss: 0.0613 - val_mae: 0.1957\n",
      "Epoch 61/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0463 - mae: 0.1706 - val_loss: 0.0554 - val_mae: 0.1896\n",
      "Epoch 62/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0501 - mae: 0.1759 - val_loss: 0.0625 - val_mae: 0.2017\n",
      "Epoch 63/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0477 - mae: 0.1720 - val_loss: 0.0553 - val_mae: 0.1878\n",
      "Epoch 64/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0459 - mae: 0.1708 - val_loss: 0.0519 - val_mae: 0.1803\n",
      "Epoch 65/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0472 - mae: 0.1709 - val_loss: 0.0563 - val_mae: 0.1911\n",
      "Epoch 66/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0491 - mae: 0.1747 - val_loss: 0.0571 - val_mae: 0.1904\n",
      "Epoch 67/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0464 - mae: 0.1705 - val_loss: 0.0541 - val_mae: 0.1831\n",
      "Epoch 68/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0456 - mae: 0.1675 - val_loss: 0.0556 - val_mae: 0.1903\n",
      "Epoch 69/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0414 - mae: 0.1595 - val_loss: 0.0522 - val_mae: 0.1762\n",
      "Epoch 70/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1631 - val_loss: 0.0533 - val_mae: 0.1855\n",
      "Epoch 71/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0430 - mae: 0.1603 - val_loss: 0.0591 - val_mae: 0.1908\n",
      "Epoch 72/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0420 - mae: 0.1612 - val_loss: 0.0552 - val_mae: 0.1865\n",
      "Epoch 73/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0458 - mae: 0.1671 - val_loss: 0.0521 - val_mae: 0.1823\n",
      "Epoch 74/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0453 - mae: 0.1640 - val_loss: 0.0511 - val_mae: 0.1798\n",
      "Epoch 75/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0456 - mae: 0.1685 - val_loss: 0.0498 - val_mae: 0.1743\n",
      "Epoch 76/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0435 - mae: 0.1659 - val_loss: 0.0660 - val_mae: 0.2023\n",
      "Epoch 77/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1632 - val_loss: 0.0622 - val_mae: 0.2051\n",
      "Epoch 78/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0449 - mae: 0.1664 - val_loss: 0.0497 - val_mae: 0.1767\n",
      "Epoch 79/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0456 - mae: 0.1691 - val_loss: 0.0538 - val_mae: 0.1884\n",
      "Epoch 80/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0480 - mae: 0.1725 - val_loss: 0.0588 - val_mae: 0.1947\n",
      "Epoch 81/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0414 - mae: 0.1606 - val_loss: 0.0580 - val_mae: 0.1918\n",
      "Epoch 82/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0450 - mae: 0.1631 - val_loss: 0.0517 - val_mae: 0.1782\n",
      "Epoch 83/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0442 - mae: 0.1653 - val_loss: 0.0494 - val_mae: 0.1747\n",
      "Epoch 84/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0472 - mae: 0.1699 - val_loss: 0.0492 - val_mae: 0.1724\n",
      "Epoch 85/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0418 - mae: 0.1598 - val_loss: 0.0509 - val_mae: 0.1794\n",
      "Epoch 86/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0450 - mae: 0.1652 - val_loss: 0.0505 - val_mae: 0.1784\n",
      "Epoch 87/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0433 - mae: 0.1637 - val_loss: 0.0563 - val_mae: 0.1817\n",
      "Epoch 88/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0398 - mae: 0.1576 - val_loss: 0.0576 - val_mae: 0.1876\n",
      "Epoch 89/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0427 - mae: 0.1615 - val_loss: 0.0527 - val_mae: 0.1825\n",
      "Epoch 90/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0448 - mae: 0.1655 - val_loss: 0.0537 - val_mae: 0.1837\n",
      "Epoch 91/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0445 - mae: 0.1681 - val_loss: 0.0495 - val_mae: 0.1750\n",
      "Epoch 92/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0403 - mae: 0.1572 - val_loss: 0.0510 - val_mae: 0.1743\n",
      "Epoch 93/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0449 - mae: 0.1676 - val_loss: 0.0502 - val_mae: 0.1759\n",
      "Epoch 94/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0431 - mae: 0.1635 - val_loss: 0.0546 - val_mae: 0.1902\n",
      "Epoch 95/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0409 - mae: 0.1576 - val_loss: 0.0557 - val_mae: 0.1854\n",
      "Epoch 96/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0418 - mae: 0.1615 - val_loss: 0.0511 - val_mae: 0.1765\n",
      "Epoch 97/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0418 - mae: 0.1583 - val_loss: 0.0513 - val_mae: 0.1760\n",
      "Epoch 98/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0421 - mae: 0.1602 - val_loss: 0.0545 - val_mae: 0.1897\n",
      "Epoch 99/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0390 - mae: 0.1557 - val_loss: 0.0504 - val_mae: 0.1774\n",
      "Epoch 100/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0380 - mae: 0.1547 - val_loss: 0.0507 - val_mae: 0.1768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e0a5045b10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del modelo 2 \n",
    "\n",
    "E2.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0534 - mae: 0.1711 \n",
      "Valores de pérdida modelo 2: [0.05030996352434158, 0.16920119524002075]\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo 2\n",
    "\n",
    "eval2 = E2.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Valores de pérdida modelo 2: {eval2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3: Add a dropout layer after each Dense Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: a single dense hidden layer\n",
    "\n",
    "E3 = Sequential()\n",
    "\n",
    "# Agregar capa de entrada con 64 unidades y activación Relu y dimensión de entrada = 7\n",
    "\n",
    "E3.add(Dense(64, activation=\"relu\", input_dim = 7))\n",
    "\n",
    "# Agregar capa densa oculta 1 con 32 unidades y función de activación Relu\n",
    "\n",
    "E3.add(Dense(32, activation=\"relu\")) # dense hidden layer 1\n",
    "\n",
    "# Agregar una capa de dropout después del dense hidden layer 1\n",
    "\n",
    "E3.add(Dropout(0.5)) # dropout layer after dense hidden layer 1\n",
    "\n",
    "# Agregar capa densa oculta 2 con 16 unidades y función de activación Relu\n",
    "\n",
    "E3.add(Dense(16, activation=\"relu\")) # dense hidden layer 2\n",
    "\n",
    "# Agregar capa de dropout después del dense hidden layer 2\n",
    "\n",
    "E3.add(Dropout(0.5)) # dropout layer after dense hidden layer 2\n",
    "\n",
    "# Agregar capa densa oculta 3 con 8 unidades y función de activación Relu\n",
    "\n",
    "E3.add(Dense(8, activation=\"relu\")) # dense hidden layer 3\n",
    "\n",
    "# Agregar capa de dropout después del dense hidden layer 3\n",
    "\n",
    "E3.add(Dropout(0.5)) # dropout layer after dense hidden layer 3\n",
    "\n",
    "# Añadir capa de salida con 1 unidad \n",
    "\n",
    "E3.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo 3 (con dropout layer después de cada hidden layer)\n",
    "\n",
    "E3.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 17.4613 - mae: 3.0271 - val_loss: 2.7426 - val_mae: 1.4520\n",
      "Epoch 2/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0943 - mae: 1.4541 - val_loss: 1.8519 - val_mae: 1.1751\n",
      "Epoch 3/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1910 - mae: 1.1876 - val_loss: 1.3071 - val_mae: 0.9670\n",
      "Epoch 4/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6676 - mae: 1.0221 - val_loss: 1.0046 - val_mae: 0.8487\n",
      "Epoch 5/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4910 - mae: 0.9591 - val_loss: 0.8829 - val_mae: 0.7992\n",
      "Epoch 6/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1649 - mae: 0.8479 - val_loss: 0.9469 - val_mae: 0.8215\n",
      "Epoch 7/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1817 - mae: 0.8428 - val_loss: 0.8188 - val_mae: 0.7659\n",
      "Epoch 8/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1083 - mae: 0.8395 - val_loss: 0.5884 - val_mae: 0.6489\n",
      "Epoch 9/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0054 - mae: 0.7809 - val_loss: 0.7008 - val_mae: 0.7107\n",
      "Epoch 10/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9831 - mae: 0.7736 - val_loss: 0.5494 - val_mae: 0.6385\n",
      "Epoch 11/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9273 - mae: 0.7525 - val_loss: 0.6844 - val_mae: 0.7050\n",
      "Epoch 12/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8844 - mae: 0.7408 - val_loss: 0.5810 - val_mae: 0.6404\n",
      "Epoch 13/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7949 - mae: 0.6893 - val_loss: 0.4390 - val_mae: 0.5566\n",
      "Epoch 14/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7574 - mae: 0.6810 - val_loss: 0.4914 - val_mae: 0.5955\n",
      "Epoch 15/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7478 - mae: 0.6708 - val_loss: 0.5740 - val_mae: 0.6393\n",
      "Epoch 16/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7565 - mae: 0.6839 - val_loss: 0.4611 - val_mae: 0.5742\n",
      "Epoch 17/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7230 - mae: 0.6616 - val_loss: 0.4653 - val_mae: 0.5687\n",
      "Epoch 18/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6341 - mae: 0.6210 - val_loss: 0.5572 - val_mae: 0.6321\n",
      "Epoch 19/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5784 - mae: 0.5922 - val_loss: 0.3470 - val_mae: 0.4944\n",
      "Epoch 20/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6608 - mae: 0.6229 - val_loss: 0.4524 - val_mae: 0.5642\n",
      "Epoch 21/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6123 - mae: 0.6170 - val_loss: 0.5279 - val_mae: 0.6119\n",
      "Epoch 22/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5612 - mae: 0.5921 - val_loss: 0.4531 - val_mae: 0.5699\n",
      "Epoch 23/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5613 - mae: 0.5909 - val_loss: 0.3673 - val_mae: 0.5102\n",
      "Epoch 24/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5932 - mae: 0.6092 - val_loss: 0.4927 - val_mae: 0.5918\n",
      "Epoch 25/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5053 - mae: 0.5549 - val_loss: 0.4421 - val_mae: 0.5565\n",
      "Epoch 26/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6065 - mae: 0.6062 - val_loss: 0.4432 - val_mae: 0.5575\n",
      "Epoch 27/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5229 - mae: 0.5615 - val_loss: 0.4949 - val_mae: 0.6003\n",
      "Epoch 28/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5128 - mae: 0.5570 - val_loss: 0.4529 - val_mae: 0.5660\n",
      "Epoch 29/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5265 - mae: 0.5634 - val_loss: 0.4570 - val_mae: 0.5683\n",
      "Epoch 30/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4872 - mae: 0.5498 - val_loss: 0.4612 - val_mae: 0.5728\n",
      "Epoch 31/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5006 - mae: 0.5527 - val_loss: 0.4505 - val_mae: 0.5624\n",
      "Epoch 32/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4827 - mae: 0.5526 - val_loss: 0.4664 - val_mae: 0.5736\n",
      "Epoch 33/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4745 - mae: 0.5464 - val_loss: 0.4684 - val_mae: 0.5704\n",
      "Epoch 34/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4587 - mae: 0.5287 - val_loss: 0.4125 - val_mae: 0.5373\n",
      "Epoch 35/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4936 - mae: 0.5554 - val_loss: 0.4775 - val_mae: 0.5827\n",
      "Epoch 36/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4563 - mae: 0.5320 - val_loss: 0.4801 - val_mae: 0.5727\n",
      "Epoch 37/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5336 - mae: 0.5680 - val_loss: 0.4391 - val_mae: 0.5507\n",
      "Epoch 38/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5011 - mae: 0.5539 - val_loss: 0.4677 - val_mae: 0.5710\n",
      "Epoch 39/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4571 - mae: 0.5287 - val_loss: 0.4468 - val_mae: 0.5668\n",
      "Epoch 40/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4782 - mae: 0.5480 - val_loss: 0.4329 - val_mae: 0.5480\n",
      "Epoch 41/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4610 - mae: 0.5335 - val_loss: 0.3942 - val_mae: 0.5238\n",
      "Epoch 42/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4603 - mae: 0.5228 - val_loss: 0.4282 - val_mae: 0.5454\n",
      "Epoch 43/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4868 - mae: 0.5469 - val_loss: 0.3877 - val_mae: 0.5169\n",
      "Epoch 44/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4752 - mae: 0.5400 - val_loss: 0.4047 - val_mae: 0.5356\n",
      "Epoch 45/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4662 - mae: 0.5287 - val_loss: 0.4487 - val_mae: 0.5646\n",
      "Epoch 46/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4392 - mae: 0.5207 - val_loss: 0.4431 - val_mae: 0.5573\n",
      "Epoch 47/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4081 - mae: 0.5029 - val_loss: 0.4224 - val_mae: 0.5431\n",
      "Epoch 48/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4686 - mae: 0.5362 - val_loss: 0.4092 - val_mae: 0.5320\n",
      "Epoch 49/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4359 - mae: 0.5146 - val_loss: 0.3812 - val_mae: 0.5139\n",
      "Epoch 50/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4422 - mae: 0.5199 - val_loss: 0.4247 - val_mae: 0.5412\n",
      "Epoch 51/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4723 - mae: 0.5394 - val_loss: 0.3969 - val_mae: 0.5275\n",
      "Epoch 52/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4428 - mae: 0.5227 - val_loss: 0.4113 - val_mae: 0.5323\n",
      "Epoch 53/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4091 - mae: 0.4940 - val_loss: 0.3711 - val_mae: 0.5081\n",
      "Epoch 54/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4228 - mae: 0.4978 - val_loss: 0.3949 - val_mae: 0.5192\n",
      "Epoch 55/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4301 - mae: 0.5097 - val_loss: 0.4087 - val_mae: 0.5315\n",
      "Epoch 56/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4404 - mae: 0.5200 - val_loss: 0.4066 - val_mae: 0.5288\n",
      "Epoch 57/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4436 - mae: 0.5159 - val_loss: 0.4711 - val_mae: 0.5697\n",
      "Epoch 58/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4383 - mae: 0.5200 - val_loss: 0.3831 - val_mae: 0.5137\n",
      "Epoch 59/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4439 - mae: 0.5252 - val_loss: 0.4089 - val_mae: 0.5316\n",
      "Epoch 60/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3765 - mae: 0.4783 - val_loss: 0.3867 - val_mae: 0.5171\n",
      "Epoch 61/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4060 - mae: 0.4919 - val_loss: 0.3481 - val_mae: 0.4884\n",
      "Epoch 62/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4170 - mae: 0.4922 - val_loss: 0.3552 - val_mae: 0.4916\n",
      "Epoch 63/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4282 - mae: 0.5094 - val_loss: 0.3170 - val_mae: 0.4676\n",
      "Epoch 64/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4169 - mae: 0.5011 - val_loss: 0.3012 - val_mae: 0.4581\n",
      "Epoch 65/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4178 - mae: 0.5020 - val_loss: 0.3378 - val_mae: 0.4884\n",
      "Epoch 66/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3855 - mae: 0.4836 - val_loss: 0.3261 - val_mae: 0.4738\n",
      "Epoch 67/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4108 - mae: 0.4949 - val_loss: 0.3661 - val_mae: 0.4966\n",
      "Epoch 68/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4158 - mae: 0.5017 - val_loss: 0.3363 - val_mae: 0.4759\n",
      "Epoch 69/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3855 - mae: 0.4848 - val_loss: 0.3420 - val_mae: 0.4845\n",
      "Epoch 70/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3949 - mae: 0.4873 - val_loss: 0.3285 - val_mae: 0.4826\n",
      "Epoch 71/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3776 - mae: 0.4770 - val_loss: 0.3266 - val_mae: 0.4751\n",
      "Epoch 72/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4301 - mae: 0.5140 - val_loss: 0.2847 - val_mae: 0.4380\n",
      "Epoch 73/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3802 - mae: 0.4684 - val_loss: 0.3325 - val_mae: 0.4778\n",
      "Epoch 74/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3957 - mae: 0.4903 - val_loss: 0.3401 - val_mae: 0.4786\n",
      "Epoch 75/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3905 - mae: 0.4834 - val_loss: 0.3318 - val_mae: 0.4764\n",
      "Epoch 76/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3714 - mae: 0.4691 - val_loss: 0.2974 - val_mae: 0.4524\n",
      "Epoch 77/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3658 - mae: 0.4635 - val_loss: 0.2959 - val_mae: 0.4469\n",
      "Epoch 78/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3624 - mae: 0.4643 - val_loss: 0.2709 - val_mae: 0.4294\n",
      "Epoch 79/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3831 - mae: 0.4664 - val_loss: 0.3355 - val_mae: 0.4882\n",
      "Epoch 80/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3726 - mae: 0.4763 - val_loss: 0.3156 - val_mae: 0.4648\n",
      "Epoch 81/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3640 - mae: 0.4763 - val_loss: 0.3540 - val_mae: 0.4932\n",
      "Epoch 82/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3784 - mae: 0.4787 - val_loss: 0.2849 - val_mae: 0.4417\n",
      "Epoch 83/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3935 - mae: 0.4850 - val_loss: 0.2509 - val_mae: 0.4112\n",
      "Epoch 84/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3851 - mae: 0.4918 - val_loss: 0.2667 - val_mae: 0.4282\n",
      "Epoch 85/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3585 - mae: 0.4651 - val_loss: 0.2882 - val_mae: 0.4440\n",
      "Epoch 86/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3525 - mae: 0.4514 - val_loss: 0.2479 - val_mae: 0.4109\n",
      "Epoch 87/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3686 - mae: 0.4558 - val_loss: 0.2656 - val_mae: 0.4268\n",
      "Epoch 88/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3623 - mae: 0.4580 - val_loss: 0.2861 - val_mae: 0.4424\n",
      "Epoch 89/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3598 - mae: 0.4656 - val_loss: 0.2543 - val_mae: 0.4142\n",
      "Epoch 90/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3987 - mae: 0.4838 - val_loss: 0.2885 - val_mae: 0.4423\n",
      "Epoch 91/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3363 - mae: 0.4464 - val_loss: 0.3217 - val_mae: 0.4738\n",
      "Epoch 92/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3721 - mae: 0.4635 - val_loss: 0.2913 - val_mae: 0.4497\n",
      "Epoch 93/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3660 - mae: 0.4707 - val_loss: 0.2809 - val_mae: 0.4387\n",
      "Epoch 94/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3879 - mae: 0.4859 - val_loss: 0.2808 - val_mae: 0.4437\n",
      "Epoch 95/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3505 - mae: 0.4568 - val_loss: 0.2955 - val_mae: 0.4534\n",
      "Epoch 96/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3766 - mae: 0.4772 - val_loss: 0.2869 - val_mae: 0.4462\n",
      "Epoch 97/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3657 - mae: 0.4559 - val_loss: 0.3033 - val_mae: 0.4594\n",
      "Epoch 98/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3869 - mae: 0.4830 - val_loss: 0.2785 - val_mae: 0.4408\n",
      "Epoch 99/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3674 - mae: 0.4756 - val_loss: 0.2865 - val_mae: 0.4461\n",
      "Epoch 100/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3459 - mae: 0.4598 - val_loss: 0.3096 - val_mae: 0.4606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e0a6b6abc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del modelo 3\n",
    "\n",
    "E3.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2755 - mae: 0.4203 \n",
      "Valores de pérdida modelo 3: [0.27483677864074707, 0.41845667362213135]\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo 3\n",
    "\n",
    "eval3 = E3.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Valores de pérdida modelo 3: {eval3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 4: Add a Batch Normalization Layer after each Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: a single dense hidden layer\n",
    "\n",
    "E4 = Sequential()\n",
    "\n",
    "# Agregar capa de entrada con 64 unidades y activación Relu y dimensión de entrada = 7\n",
    "\n",
    "E4.add(Dense(64, activation=\"relu\", input_dim = 7))\n",
    "\n",
    "# Agregar capa densa oculta 1 con 32 unidades y función de activación Relu\n",
    "\n",
    "E4.add(Dense(32, activation=\"relu\")) # dense hidden layer 1\n",
    "\n",
    "# Agregar una capa de dropout después del dense hidden layer 1\n",
    "\n",
    "E4.add(Dropout(0.5)) # dropout layer after dense hidden layer 1\n",
    "\n",
    "# Agregar batch normalization layer después del dropout layer 1\n",
    "\n",
    "E4.add(BatchNormalization()) # Batch Normalization Layer after Dropout Layer 1\n",
    "\n",
    "# Agregar capa densa oculta 2 con 16 unidades y función de activación Relu\n",
    "\n",
    "E4.add(Dense(16, activation=\"relu\")) # dense hidden layer 2\n",
    "\n",
    "# Agregar capa de dropout después del dense hidden layer 2\n",
    "\n",
    "E4.add(Dropout(0.5)) # dropout layer after dense hidden layer 2\n",
    "\n",
    "# Agregar batch normalization layer después del dropout layer 2\n",
    "\n",
    "E4.add(BatchNormalization()) # Batch Normalization Layer after Dropout Layer 2\n",
    "\n",
    "# Agregar capa densa oculta 3 con 8 unidades y función de activación Relu\n",
    "\n",
    "E4.add(Dense(8, activation=\"relu\")) # dense hidden layer 3\n",
    "\n",
    "# Agregar capa de dropout después del dense hidden layer 3\n",
    "\n",
    "E4.add(Dropout(0.5)) # dropout layer after dense hidden layer 3\n",
    "\n",
    "# Agregar batch normalization layer después del dropout layer 3\n",
    "\n",
    "E4.add(BatchNormalization()) # Batch Normalization Layer after Dropout Layer 3\n",
    "\n",
    "# Añadir capa de salida con 1 unidad \n",
    "\n",
    "E4.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo 4 (con batchnormalization después de cada dropout layer)\n",
    "\n",
    "E4.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 4.9914 - mae: 1.9054 - val_loss: 2.9514 - val_mae: 1.5834\n",
      "Epoch 2/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7381 - mae: 1.3778 - val_loss: 1.5947 - val_mae: 1.1185\n",
      "Epoch 3/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6927 - mae: 1.0570 - val_loss: 0.6834 - val_mae: 0.7019\n",
      "Epoch 4/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2140 - mae: 0.8828 - val_loss: 0.5100 - val_mae: 0.6068\n",
      "Epoch 5/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9923 - mae: 0.8036 - val_loss: 0.4442 - val_mae: 0.5673\n",
      "Epoch 6/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9045 - mae: 0.7581 - val_loss: 0.4991 - val_mae: 0.6043\n",
      "Epoch 7/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8158 - mae: 0.7293 - val_loss: 0.4318 - val_mae: 0.5564\n",
      "Epoch 8/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7120 - mae: 0.6852 - val_loss: 0.4299 - val_mae: 0.5555\n",
      "Epoch 9/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7332 - mae: 0.6874 - val_loss: 0.5006 - val_mae: 0.6028\n",
      "Epoch 10/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6511 - mae: 0.6519 - val_loss: 0.3818 - val_mae: 0.5209\n",
      "Epoch 11/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6933 - mae: 0.6790 - val_loss: 0.4209 - val_mae: 0.5493\n",
      "Epoch 12/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6396 - mae: 0.6489 - val_loss: 0.3371 - val_mae: 0.4721\n",
      "Epoch 13/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5473 - mae: 0.5988 - val_loss: 0.3544 - val_mae: 0.4967\n",
      "Epoch 14/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5883 - mae: 0.6174 - val_loss: 0.3739 - val_mae: 0.5200\n",
      "Epoch 15/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.6136 - mae: 0.6288 - val_loss: 0.3647 - val_mae: 0.5127\n",
      "Epoch 16/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5571 - mae: 0.6114 - val_loss: 0.3189 - val_mae: 0.4749\n",
      "Epoch 17/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5606 - mae: 0.6050 - val_loss: 0.3338 - val_mae: 0.4868\n",
      "Epoch 18/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5313 - mae: 0.5920 - val_loss: 0.3068 - val_mae: 0.4692\n",
      "Epoch 19/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5733 - mae: 0.6036 - val_loss: 0.2583 - val_mae: 0.4169\n",
      "Epoch 20/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5525 - mae: 0.6008 - val_loss: 0.2762 - val_mae: 0.4371\n",
      "Epoch 21/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5193 - mae: 0.5851 - val_loss: 0.2546 - val_mae: 0.4129\n",
      "Epoch 22/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5170 - mae: 0.5841 - val_loss: 0.2803 - val_mae: 0.4408\n",
      "Epoch 23/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4715 - mae: 0.5461 - val_loss: 0.2666 - val_mae: 0.4263\n",
      "Epoch 24/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5046 - mae: 0.5742 - val_loss: 0.2537 - val_mae: 0.4153\n",
      "Epoch 25/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5067 - mae: 0.5756 - val_loss: 0.2246 - val_mae: 0.3752\n",
      "Epoch 26/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4393 - mae: 0.5327 - val_loss: 0.2514 - val_mae: 0.4011\n",
      "Epoch 27/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5006 - mae: 0.5631 - val_loss: 0.2875 - val_mae: 0.4461\n",
      "Epoch 28/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4986 - mae: 0.5672 - val_loss: 0.2272 - val_mae: 0.3966\n",
      "Epoch 29/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5207 - mae: 0.5757 - val_loss: 0.2224 - val_mae: 0.3888\n",
      "Epoch 30/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5198 - mae: 0.5828 - val_loss: 0.2217 - val_mae: 0.3723\n",
      "Epoch 31/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5199 - mae: 0.5804 - val_loss: 0.1958 - val_mae: 0.3546\n",
      "Epoch 32/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4444 - mae: 0.5252 - val_loss: 0.2046 - val_mae: 0.3651\n",
      "Epoch 33/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4687 - mae: 0.5522 - val_loss: 0.1943 - val_mae: 0.3603\n",
      "Epoch 34/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4281 - mae: 0.5196 - val_loss: 0.2418 - val_mae: 0.4005\n",
      "Epoch 35/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4914 - mae: 0.5638 - val_loss: 0.2756 - val_mae: 0.4320\n",
      "Epoch 36/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4568 - mae: 0.5408 - val_loss: 0.1887 - val_mae: 0.3485\n",
      "Epoch 37/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4502 - mae: 0.5348 - val_loss: 0.2223 - val_mae: 0.3857\n",
      "Epoch 38/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4434 - mae: 0.5214 - val_loss: 0.2117 - val_mae: 0.3771\n",
      "Epoch 39/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4531 - mae: 0.5381 - val_loss: 0.2142 - val_mae: 0.3813\n",
      "Epoch 40/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4494 - mae: 0.5400 - val_loss: 0.2055 - val_mae: 0.3731\n",
      "Epoch 41/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4595 - mae: 0.5425 - val_loss: 0.1974 - val_mae: 0.3524\n",
      "Epoch 42/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4691 - mae: 0.5525 - val_loss: 0.1622 - val_mae: 0.3289\n",
      "Epoch 43/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4554 - mae: 0.5458 - val_loss: 0.1996 - val_mae: 0.3668\n",
      "Epoch 44/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4775 - mae: 0.5550 - val_loss: 0.2011 - val_mae: 0.3538\n",
      "Epoch 45/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4789 - mae: 0.5598 - val_loss: 0.2156 - val_mae: 0.3915\n",
      "Epoch 46/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4425 - mae: 0.5381 - val_loss: 0.2308 - val_mae: 0.3922\n",
      "Epoch 47/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4409 - mae: 0.5294 - val_loss: 0.1393 - val_mae: 0.3042\n",
      "Epoch 48/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4581 - mae: 0.5450 - val_loss: 0.1602 - val_mae: 0.3288\n",
      "Epoch 49/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4369 - mae: 0.5355 - val_loss: 0.1912 - val_mae: 0.3656\n",
      "Epoch 50/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4466 - mae: 0.5383 - val_loss: 0.1632 - val_mae: 0.3342\n",
      "Epoch 51/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4276 - mae: 0.5251 - val_loss: 0.1795 - val_mae: 0.3529\n",
      "Epoch 52/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4508 - mae: 0.5454 - val_loss: 0.1886 - val_mae: 0.3562\n",
      "Epoch 53/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4853 - mae: 0.5609 - val_loss: 0.1669 - val_mae: 0.3344\n",
      "Epoch 54/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4146 - mae: 0.5106 - val_loss: 0.1417 - val_mae: 0.3088\n",
      "Epoch 55/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4250 - mae: 0.5179 - val_loss: 0.1852 - val_mae: 0.3522\n",
      "Epoch 56/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4111 - mae: 0.5098 - val_loss: 0.1607 - val_mae: 0.3327\n",
      "Epoch 57/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4303 - mae: 0.5312 - val_loss: 0.1928 - val_mae: 0.3698\n",
      "Epoch 58/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4155 - mae: 0.5132 - val_loss: 0.1823 - val_mae: 0.3559\n",
      "Epoch 59/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4476 - mae: 0.5266 - val_loss: 0.1605 - val_mae: 0.3314\n",
      "Epoch 60/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4169 - mae: 0.5138 - val_loss: 0.2047 - val_mae: 0.3673\n",
      "Epoch 61/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4221 - mae: 0.5250 - val_loss: 0.2461 - val_mae: 0.4126\n",
      "Epoch 62/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3931 - mae: 0.4993 - val_loss: 0.1653 - val_mae: 0.3409\n",
      "Epoch 63/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4262 - mae: 0.5280 - val_loss: 0.1633 - val_mae: 0.3367\n",
      "Epoch 64/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4139 - mae: 0.5160 - val_loss: 0.1565 - val_mae: 0.3291\n",
      "Epoch 65/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4177 - mae: 0.5243 - val_loss: 0.1950 - val_mae: 0.3690\n",
      "Epoch 66/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4650 - mae: 0.5444 - val_loss: 0.2064 - val_mae: 0.3648\n",
      "Epoch 67/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3892 - mae: 0.5019 - val_loss: 0.1811 - val_mae: 0.3596\n",
      "Epoch 68/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4024 - mae: 0.5098 - val_loss: 0.1628 - val_mae: 0.3360\n",
      "Epoch 69/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4273 - mae: 0.5209 - val_loss: 0.1534 - val_mae: 0.3253\n",
      "Epoch 70/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4434 - mae: 0.5379 - val_loss: 0.1474 - val_mae: 0.3189\n",
      "Epoch 71/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4577 - mae: 0.5314 - val_loss: 0.2343 - val_mae: 0.3999\n",
      "Epoch 72/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4289 - mae: 0.5260 - val_loss: 0.1573 - val_mae: 0.3280\n",
      "Epoch 73/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4070 - mae: 0.5089 - val_loss: 0.1614 - val_mae: 0.3331\n",
      "Epoch 74/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4415 - mae: 0.5327 - val_loss: 0.1915 - val_mae: 0.3661\n",
      "Epoch 75/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3973 - mae: 0.5025 - val_loss: 0.2222 - val_mae: 0.3852\n",
      "Epoch 76/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4259 - mae: 0.5236 - val_loss: 0.2107 - val_mae: 0.3635\n",
      "Epoch 77/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4554 - mae: 0.5321 - val_loss: 0.2134 - val_mae: 0.3898\n",
      "Epoch 78/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4445 - mae: 0.5301 - val_loss: 0.2585 - val_mae: 0.4066\n",
      "Epoch 79/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4400 - mae: 0.5312 - val_loss: 0.2406 - val_mae: 0.4128\n",
      "Epoch 80/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4563 - mae: 0.5366 - val_loss: 0.2493 - val_mae: 0.4141\n",
      "Epoch 81/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4231 - mae: 0.5250 - val_loss: 0.5196 - val_mae: 0.4668\n",
      "Epoch 82/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4613 - mae: 0.5424 - val_loss: 0.3526 - val_mae: 0.4297\n",
      "Epoch 83/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4272 - mae: 0.5211 - val_loss: 0.3612 - val_mae: 0.3669\n",
      "Epoch 84/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4186 - mae: 0.5209 - val_loss: 0.3535 - val_mae: 0.4265\n",
      "Epoch 85/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4547 - mae: 0.5459 - val_loss: 0.3744 - val_mae: 0.4614\n",
      "Epoch 86/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4284 - mae: 0.5245 - val_loss: 0.1992 - val_mae: 0.3611\n",
      "Epoch 87/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3901 - mae: 0.4985 - val_loss: 0.1759 - val_mae: 0.3493\n",
      "Epoch 88/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4212 - mae: 0.5263 - val_loss: 0.2209 - val_mae: 0.3872\n",
      "Epoch 89/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4074 - mae: 0.5080 - val_loss: 0.1568 - val_mae: 0.3325\n",
      "Epoch 90/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3947 - mae: 0.5027 - val_loss: 0.1540 - val_mae: 0.3263\n",
      "Epoch 91/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3874 - mae: 0.5030 - val_loss: 0.1369 - val_mae: 0.3085\n",
      "Epoch 92/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3997 - mae: 0.4923 - val_loss: 0.1516 - val_mae: 0.3282\n",
      "Epoch 93/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4509 - mae: 0.5423 - val_loss: 0.1441 - val_mae: 0.3155\n",
      "Epoch 94/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4407 - mae: 0.5257 - val_loss: 0.1663 - val_mae: 0.3423\n",
      "Epoch 95/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4165 - mae: 0.5094 - val_loss: 0.1674 - val_mae: 0.3435\n",
      "Epoch 96/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3848 - mae: 0.5036 - val_loss: 0.2043 - val_mae: 0.3821\n",
      "Epoch 97/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3682 - mae: 0.4741 - val_loss: 0.2476 - val_mae: 0.4121\n",
      "Epoch 98/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4588 - mae: 0.5417 - val_loss: 0.2673 - val_mae: 0.4418\n",
      "Epoch 99/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4076 - mae: 0.5082 - val_loss: 0.1895 - val_mae: 0.3654\n",
      "Epoch 100/100\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4258 - mae: 0.5286 - val_loss: 0.2669 - val_mae: 0.4417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e0a8f154b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del modelo 4\n",
    "\n",
    "E4.fit(X_train, y_train, epochs=100, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2264 - mae: 0.4028 \n",
      "Valores de pérdida modelo 4: [0.24065794050693512, 0.4098511040210724]\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo 4\n",
    "\n",
    "eval4 = E4.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Valores de pérdida modelo 4: {eval4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Predictions with the 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_model1 = E1.predict(X_test) # predictions with model 1 (single dense layer)\n",
    "\n",
    "pred_model2 = E2.predict(X_test) # predictions with model 2 (3 dense hidden layers)\n",
    "\n",
    "pred_model3 = E3.predict(X_test) # predictions with model 3 (3 Dense hidden layers + dropout)\n",
    "\n",
    "pred_model4 = E4.predict(X_test) # predictions with model 4 (Dense hidden layers + dropout + batch normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Tabla comparativa de valores reales de GPA contra predicciones de los 4 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model1 = pd.Series(np.reshape(pred_model1, len(pred_model1)))\n",
    "pred_model1.index = y_test.index\n",
    "\n",
    "pred_model2 = pd.Series(np.reshape(pred_model2, len(pred_model2)))\n",
    "pred_model2.index = y_test.index\n",
    "\n",
    "pred_model3 = pd.Series(np.reshape(pred_model3, len(pred_model3)))\n",
    "pred_model3.index = y_test.index\n",
    "\n",
    "pred_model4 = pd.Series(np.reshape(pred_model4, len(pred_model4)))\n",
    "pred_model4.index = y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo 1</th>\n",
       "      <th>Modelo 2</th>\n",
       "      <th>Modelo 3</th>\n",
       "      <th>Modelo 4</th>\n",
       "      <th>GPA real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1.362935</td>\n",
       "      <td>1.535858</td>\n",
       "      <td>1.487540</td>\n",
       "      <td>1.663879</td>\n",
       "      <td>1.427724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3.048964</td>\n",
       "      <td>3.150845</td>\n",
       "      <td>2.423644</td>\n",
       "      <td>2.798783</td>\n",
       "      <td>3.117354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2342</th>\n",
       "      <td>2.081095</td>\n",
       "      <td>2.187081</td>\n",
       "      <td>1.970699</td>\n",
       "      <td>1.885336</td>\n",
       "      <td>2.037769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>3.716757</td>\n",
       "      <td>3.860939</td>\n",
       "      <td>2.666003</td>\n",
       "      <td>4.180348</td>\n",
       "      <td>3.548521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.451198</td>\n",
       "      <td>0.494626</td>\n",
       "      <td>1.243160</td>\n",
       "      <td>1.042828</td>\n",
       "      <td>0.248977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1.853950</td>\n",
       "      <td>1.775873</td>\n",
       "      <td>1.997903</td>\n",
       "      <td>1.894092</td>\n",
       "      <td>1.562360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2.074698</td>\n",
       "      <td>2.103497</td>\n",
       "      <td>2.024846</td>\n",
       "      <td>1.915603</td>\n",
       "      <td>2.174903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>2.169771</td>\n",
       "      <td>2.261521</td>\n",
       "      <td>2.133455</td>\n",
       "      <td>1.925295</td>\n",
       "      <td>2.332540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>2.725360</td>\n",
       "      <td>2.729002</td>\n",
       "      <td>2.193378</td>\n",
       "      <td>1.917401</td>\n",
       "      <td>2.777967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>0.986181</td>\n",
       "      <td>0.859922</td>\n",
       "      <td>1.364492</td>\n",
       "      <td>1.509829</td>\n",
       "      <td>0.863545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Modelo 1  Modelo 2  Modelo 3  Modelo 4  GPA real\n",
       "1004  1.362935  1.535858  1.487540  1.663879  1.427724\n",
       "196   3.048964  3.150845  2.423644  2.798783  3.117354\n",
       "2342  2.081095  2.187081  1.970699  1.885336  2.037769\n",
       "1708  3.716757  3.860939  2.666003  4.180348  3.548521\n",
       "435   0.451198  0.494626  1.243160  1.042828  0.248977\n",
       "...        ...       ...       ...       ...       ...\n",
       "986   1.853950  1.775873  1.997903  1.894092  1.562360\n",
       "120   2.074698  2.103497  2.024846  1.915603  2.174903\n",
       "283   2.169771  2.261521  2.133455  1.925295  2.332540\n",
       "1740  2.725360  2.729002  2.193378  1.917401  2.777967\n",
       "1726  0.986181  0.859922  1.364492  1.509829  0.863545\n",
       "\n",
       "[479 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPA_realvspred = pd.DataFrame({\"Modelo 1\": pd.Series(np.reshape(pred_model1, len(pred_model1))), \n",
    "                               \"Modelo 2\": pd.Series(np.reshape(pred_model2, len(pred_model2))), \n",
    "                               \"Modelo 3\": pd.Series(np.reshape(pred_model3, len(pred_model3))), \n",
    "                               \"Modelo 4\": pd.Series(np.reshape(pred_model4, len(pred_model4))), \n",
    "                               \"GPA real\": y_test})\n",
    "\n",
    "GPA_realvspred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Tabla comparativa de métricas (MSE y MAE) de los 4 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Single Dense Hidden Layer</th>\n",
       "      <td>0.0587</td>\n",
       "      <td>0.1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 Dense Hidden Layers</th>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.1711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense + Dropout</th>\n",
       "      <td>0.2755</td>\n",
       "      <td>0.4203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense + Dropout + Batch Normalization</th>\n",
       "      <td>0.2264</td>\n",
       "      <td>0.4028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          MSE     MAE\n",
       "Modelo                                               \n",
       "Single Dense Hidden Layer              0.0587  0.1782\n",
       "3 Dense Hidden Layers                  0.0534  0.1711\n",
       "Dense + Dropout                        0.2755  0.4203\n",
       "Dense + Dropout + Batch Normalization  0.2264  0.4028"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_table = pd.DataFrame({\"MSE\": [0.0587,0.0534,0.2755,0.2264], \n",
    "                              \"MAE\":[0.1782,0.1711,0.4203,0.4028]}, \n",
    "                              index=[\"Single Dense Hidden Layer\", \"3 Dense Hidden Layers\", \"Dense + Dropout\", \"Dense + Dropout + Batch Normalization\"])\n",
    "\n",
    "# Asignar nombre al índice del dataframe anterior\n",
    "\n",
    "metrics_table.index.name = \"Modelo\"\n",
    "\n",
    "# Mostrar los valores de las métricas para los 4 modelos: valores de pérdida MSE Y MAE\n",
    "\n",
    "metrics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Conclusión final sobre el mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusión, es posible afirmar que de los 4 modelos probados anteriormente, aquel que resulta tener el mejor desempeño es el modelo 2 (aquel con 3 capas densas ocultas) y sin dropout ni batch normalization, esto principalmente debido a que éste modelo posee un valor de MSE igual a 0.0534 junto con un MAE de 0.1711, siendo éstos los menores valores posibles de MSE y MAE, de entre todos los modelos probados, lo cual es un indicativo de que el modelo con 3 capas densas ocultas (modelo 2) es aquel que arroja las predicciones con el mayor grado de precisión y confiabilidad posibles, mientras que los demás modelos, entre ellos, a los que se les aplicó batch normalization y dropout muestran tener una mayor margen de error en cuanto a su capacidad predictiva, esto principalmente porque poseen un mayor valor de MSE y de MAE que el modelo con triple capa densa oculta (3 Dense Hidden Layers), además, cabe mencionar que los modelos con el dropout y batch normalization aplicados es probable que tengan un mayor valor de MSE y MAE, debido a que el dataset de prueba para los modelos no contiene un volumen suficientemente alto de datos como para que las técnicas de dropout y batch normalization produzcan una mejoría significativa en la precisión de las predicciones de los modelos, además de que el hecho de utilizar alguna de éstas 2 técnicas, tales como el dropout, ocasiona por el contrario, que el MSE y el MAE de los modelos incremente, indicando mayor margen de error en las predicciones derivadas de los mismos, por lo que al tener una cantidad relativamente baja de datos en el dataset de prueba y aplicar técnicas como dropout y batch normalization, al momento de eliminar aleatoriamente ciertas neuronas de las redes, las predicciones de los modelos se sesgan más y por tanto pierden precisión y en consecuencia también su confiabilidad resulta ser menor, por lo que es más recomendable utilizar las técnicas de batch normalization y dropout en modelos que se pongan a prueba con una cantidad mayormente alta de datos en el dataset de prueba, esto para asegurar que al quitar neuronas de forma aleatoria mediante dropout, o bien, al aplicar batch normalization a los modelos en un intento por mejorar el desempeño de los mismos, éstos mismos modelos presenten una mejoría significativa en cuanto a su rendimiento predictivo, lo cual se traducirá en predicciones más precisas y confiables y en una mayor cantidad existente de alternativas para lograrlas, puesto que el dropout permitirá a los modelos explorar alternativas que probablemente no hayan sido exploradas anteriormente y que una de ellas sea la que arroje las predicciones más precisas y confiables posible de entre todas las posibilidades, además de que al haber muchos más datos, el quitar neuronas de los modelos tendrá un menor impacto negativo en en la precisión del mismo para realizar predicciones, por lo que las métricas como el MSE y el MAE ya no aumentarán significativamente al emplear tanto dropout como batch normalization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
